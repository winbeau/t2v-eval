# =============================================================================
# T2V-Eval Configuration - 21 Latent Frames Experiments (84 actual frames)
# 60% sampling coverage = 50 frames
# =============================================================================

# -----------------------------------------------------------------------------
# HuggingFace Dataset Configuration
# -----------------------------------------------------------------------------
dataset:
  repo_id: "hf/AdaHead"
  split: "train"
  use_local_videos: true
  local_video_dir: "hf/AdaHead/videos/Exp_OscStable_Head_Window"
  prompt_file: "hf/AdaHead/videos/Exp_OscStable_Head_Window/prompts.csv"
  video_dir: "videos/Exp_OscStable_Head_Window"

# -----------------------------------------------------------------------------
# Experiment Groups (21 latent frames only)
# 6 groups: 2 baselines + 2 oscillating + 2 stable
# -----------------------------------------------------------------------------
groups:
  # Baseline comparisons
  - name: "frame_baseline_21"
    description: "Frame-level baseline, 21 latent frames (84 actual)"
    latent_frames: 21
    actual_frames: 84
  - name: "head_baseline_21"
    description: "Head-level baseline, 21 latent frames (84 actual)"
    latent_frames: 21
    actual_frames: 84

  # Oscillating attention patterns
  - name: "osc_long_21"
    description: "Oscillating attention (long window), 21 latent frames"
    latent_frames: 21
    actual_frames: 84
  - name: "osc_short_21"
    description: "Oscillating attention (short window), 21 latent frames"
    latent_frames: 21
    actual_frames: 84

  # Stable attention patterns
  - name: "stable_long_21"
    description: "Stable attention (long window), 21 latent frames"
    latent_frames: 21
    actual_frames: 84
  - name: "stable_short_21"
    description: "Stable attention (short window), 21 latent frames"
    latent_frames: 21
    actual_frames: 84

# Group categories for analysis
group_categories:
  by_attention_type:
    frame: ["frame_baseline_21"]
    head: ["head_baseline_21"]
    oscillating: ["osc_long_21", "osc_short_21"]
    stable: ["stable_long_21", "stable_short_21"]

  by_window_size:
    long_window: ["osc_long_21", "stable_long_21"]
    short_window: ["osc_short_21", "stable_short_21"]

# -----------------------------------------------------------------------------
# Unified Evaluation Protocol
# -----------------------------------------------------------------------------
protocol:
  fps_eval: 8                    # Evaluation FPS
  num_frames: 50                 # 60% coverage of 84 actual frames
  resize: 256                    # Spatial resolution
  frame_sampling: "uniform"      # Sampling strategy: uniform

  # Frame handling for videos with different lengths
  frame_padding: "loop"          # Options: loop, repeat_last, truncate

# -----------------------------------------------------------------------------
# Metrics Configuration
# -----------------------------------------------------------------------------
metrics:
  enabled:
    - "clip_or_vqa"
    - "vbench_temporal"
    - "flicker"
    - "niqe"

  clip_or_vqa:
    mode: "clip"                 # Options: clip, vqa
    model_name: "ViT-B-32"       # CLIP model variant
    pretrained: "openai"         # Pretrained weights source
    num_frames_for_score: 50     # Match protocol sampling
    aggregation: "mean"          # How to aggregate frame scores: mean, max

  vbench:
    enabled: true
    temporal_only: true          # Focus on temporal metrics
    # Note: subject_consistency often fails with ZeroDivisionError on custom videos
    subtasks:
      - "temporal_flickering"
      - "motion_smoothness"

  flicker:
    method: "l1"                 # Difference method: l1, l2
    normalize: true              # Normalize pixel values to [0, 1]
    compute_std: true            # Also compute std of frame differences
    grayscale: false             # Convert to grayscale before computing

  niqe:
    enabled: true
    num_frames_for_niqe: 50      # Match protocol sampling
    block_size: 96               # NIQE block size
    alternative: "niqe"          # Options: niqe, brisque

# -----------------------------------------------------------------------------
# Runtime Configuration
# -----------------------------------------------------------------------------
runtime:
  device: "cuda"                 # Device: cuda, cpu
  batch_size: 1                  # Batch size for inference
  num_workers: 4                 # DataLoader workers
  seed: 42                       # Random seed for reproducibility

# -----------------------------------------------------------------------------
# Paths Configuration
# -----------------------------------------------------------------------------
paths:
  cache_dir: "./eval_cache"
  output_dir: "./outputs"
  metadata_file: "metadata.csv"
  processed_metadata: "processed_metadata.csv"
  per_video_metrics: "per_video_metrics.csv"
  group_summary: "group_summary.csv"
  runtime_csv: "runtime.csv"
  figures_dir: "./outputs/figs"

  # Custom experiment output filename
  experiment_output: "Exp_21frames_OscStable_Head_Win.csv"

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
logging:
  level: "INFO"
  log_file: "./outputs/eval.log"
  console: true
