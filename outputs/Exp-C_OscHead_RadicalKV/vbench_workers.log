WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2026-02-12 01:45:54 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-12 01:45:54 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-12 01:45:54 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-12 01:45:54 [INFO] VBench logging enabled: rank0 -> /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/run_vbench.log (other ranks -> .rankN suffix)
2026-02-12 01:45:54 [INFO] Multi-process dimension-parallel mode enabled: world_size=4 (no torch.distributed init)
2026-02-12 01:45:54 [INFO] Force recomputing: outputs/Exp-C_OscHead_RadicalKV/vbench_per_video.csv
2026-02-12 01:45:54 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-12 01:45:54 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-12 01:45:54 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-12 01:45:54 [WARNING] Remapped 100 records to unique video_id.
2026-02-12 01:45:54 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-12 01:45:54 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-12 01:45:54 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-12 01:45:54 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-12 01:45:54 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-12 01:45:54 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-12 01:45:54 [WARNING] Remapped 100 records to unique video_id.
2026-02-12 01:45:54 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-12 01:45:54 [WARNING] Remapped 100 records to unique video_id.
2026-02-12 01:45:54 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-12 01:45:54 [INFO] Loaded 100 videos for VBench evaluation
2026-02-12 01:45:54 [INFO] Subtask distribution: total=16, world_size=4
2026-02-12 01:45:54 [INFO]   rank 0 -> ['subject_consistency', 'temporal_style', 'multiple_objects', 'overall_consistency']
2026-02-12 01:45:54 [INFO]   rank 1 -> ['background_consistency', 'appearance_style', 'spatial_relationship', 'dynamic_degree']
2026-02-12 01:45:54 [INFO]   rank 2 -> ['temporal_flickering', 'scene', 'human_action', 'imaging_quality']
2026-02-12 01:45:54 [INFO]   rank 3 -> ['motion_smoothness', 'object_class', 'color', 'aesthetic_quality']
2026-02-12 01:45:54 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-12 01:45:54 [WARNING] Remapped 100 records to unique video_id.
2026-02-12 01:45:54 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-12 01:45:54 [INFO] Attempting VBench evaluation via Python API...
2026-02-12 01:45:57 [INFO] Applied compatibility shim for `moviepy.editor` (VBench-Long expects legacy import path).
2026-02-12 01:45:59 [INFO] Detected existing split clips for all videos; skip preprocessing and reuse cache.
2026-02-12 01:45:59 [INFO] Running VBench-Long evaluation with subtasks: ['subject_consistency', 'temporal_style', 'multiple_objects', 'overall_consistency']
2026-02-12 01:45:59 [INFO] Evaluating subtask: subject_consistency
2026-02-12 01:47:54 [INFO] [scene] auxiliary_info ready for 100 videos (heuristic=100)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
2026-02-12 01:52:14 [INFO] [background_consistency] prefix fallback expanded 900 unresolved entries to 100 video assignments
2026-02-12 01:52:14 [INFO] [background_consistency] skipped 900 unresolved video ids from raw VBench output
2026-02-12 01:52:14 [INFO] [appearance_style] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-12 01:53:35 [INFO] [object_class] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-12 01:53:43 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 01:53:43 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 01:54:01 [INFO] [subject_consistency] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/subject_consistency_full_info.json
2026-02-12 01:54:01 [INFO] [subject_consistency] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/subject_consistency_full_info.json
2026-02-12 01:54:01 [INFO] [subject_consistency] /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/input_videos/subject_consistency_cat_firstframes_videos has already been created, please check the path
2026-02-12 01:54:01 [INFO] [subject_consistency] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/subject_consistency_eval_results.json
2026-02-12 01:54:01 [INFO] [subject_consistency] prefix fallback expanded 900 unresolved entries to 100 video assignments
2026-02-12 01:54:01 [INFO] [subject_consistency] skipped 900 unresolved video ids from raw VBench output
2026-02-12 01:54:01 [INFO] Parsed 100 results for subject_consistency
2026-02-12 01:54:01 [INFO] Evaluating subtask: temporal_style
2026-02-12 01:58:17 [INFO] [temporal_style] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/temporal_style_full_info.json
2026-02-12 01:58:17 [INFO] [temporal_style] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/temporal_style_full_info.json
2026-02-12 01:58:17 [INFO] [temporal_style] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/temporal_style_eval_results.json
2026-02-12 01:58:18 [INFO] Parsed 100 results for temporal_style
2026-02-12 01:58:18 [INFO] Evaluating subtask: multiple_objects
2026-02-12 01:58:18 [INFO] [multiple_objects] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-12 01:58:22 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 01:58:22 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 02:03:09 [INFO] [spatial_relationship] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-12 02:03:17 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 02:03:17 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 02:16:53 [INFO] Rank 2 wrote partial results: outputs/Exp-C_OscHead_RadicalKV/vbench_partials/rank_2.csv
2026-02-12 03:31:24 [INFO] [color] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-12 03:31:31 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 03:31:31 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-12 03:40:19 [INFO] [multiple_objects] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/multiple_objects_full_info.json
2026-02-12 03:40:19 [INFO] [multiple_objects] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/multiple_objects_full_info.json
2026-02-12 03:40:19 [INFO] [multiple_objects] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/multiple_objects_eval_results.json
2026-02-12 03:40:19 [INFO] Parsed 100 results for multiple_objects
2026-02-12 03:40:19 [INFO] Evaluating subtask: overall_consistency
2026-02-12 03:45:20 [INFO] [overall_consistency] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/overall_consistency_full_info.json
2026-02-12 03:45:20 [INFO] [overall_consistency] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/overall_consistency_full_info.json
2026-02-12 03:45:20 [INFO] [overall_consistency] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/overall_consistency_eval_results.json
2026-02-12 03:45:20 [INFO] Parsed 100 results for overall_consistency
2026-02-12 03:45:20 [INFO] Rank 0 wrote partial results: outputs/Exp-C_OscHead_RadicalKV/vbench_partials/rank_0.csv
2026-02-12 04:02:43 [INFO] Rank 1 wrote partial results: outputs/Exp-C_OscHead_RadicalKV/vbench_partials/rank_1.csv
