WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2026-02-11 17:30:43 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-11 17:30:43 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-11 17:30:43 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-11 17:30:43 [INFO] VBench logging enabled: rank0 -> /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/run_vbench.log (other ranks -> .rankN suffix)
2026-02-11 17:30:43 [INFO] Multi-process dimension-parallel mode enabled: world_size=4 (no torch.distributed init)
2026-02-11 17:30:43 [INFO] Force recomputing: outputs/Exp-C_OscHead_RadicalKV/vbench_per_video.csv
2026-02-11 17:30:43 [INFO] Metadata not found, building video list from local dataset paths...
2026-02-11 17:30:43 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-11 17:30:43 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-11 17:30:43 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-11 17:30:43 [INFO] Loaded prompts from: /workspace/t2v-eval/hf/AdaHead/Exp-C_OscHead_RadicalKV/c1_self_forcing_baseline_72/prompts.csv (25 rows)
2026-02-11 17:30:43 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-11 17:30:43 [WARNING] Remapped 100 records to unique video_id.
2026-02-11 17:30:43 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-11 17:30:43 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-11 17:30:43 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-11 17:30:43 [WARNING] Remapped 100 records to unique video_id.
2026-02-11 17:30:43 [WARNING] Remapped 100 records to unique video_id.
2026-02-11 17:30:43 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-11 17:30:43 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-11 17:30:43 [WARNING] Detected duplicate video_id values across groups (25 duplicated ids). Applying group-prefixed remap for VBench run.
2026-02-11 17:30:43 [WARNING] Remapped 100 records to unique video_id.
2026-02-11 17:30:43 [INFO] Group alias mapping for duplicate IDs: {'c1_self_forcing_baseline_72': 'g1', 'c2_headkv_aligned_baseline_72': 'g2', 'c3_osc_class_windowed_select_72': 'g3', 'c4_osc_class_global_select_72': 'g4'}
2026-02-11 17:30:43 [INFO] Loaded 100 videos for VBench evaluation
2026-02-11 17:30:43 [INFO] Subtask distribution: total=16, world_size=4
2026-02-11 17:30:43 [INFO]   rank 0 -> ['subject_consistency', 'temporal_style', 'multiple_objects', 'overall_consistency']
2026-02-11 17:30:43 [INFO]   rank 1 -> ['background_consistency', 'appearance_style', 'spatial_relationship', 'dynamic_degree']
2026-02-11 17:30:43 [INFO]   rank 2 -> ['temporal_flickering', 'scene', 'human_action', 'imaging_quality']
2026-02-11 17:30:43 [INFO]   rank 3 -> ['motion_smoothness', 'object_class', 'color', 'aesthetic_quality']
2026-02-11 17:30:43 [INFO] Attempting VBench evaluation via Python API...
2026-02-11 17:30:46 [INFO] Applied compatibility shim for `moviepy.editor` (VBench-Long expects legacy import path).
2026-02-11 17:30:48 [INFO] Detected existing split clips for all videos; skip preprocessing and reuse cache.
2026-02-11 17:30:48 [INFO] Running VBench-Long evaluation with subtasks: ['subject_consistency', 'temporal_style', 'multiple_objects', 'overall_consistency']
2026-02-11 17:30:48 [INFO] Evaluating subtask: subject_consistency
2026-02-11 17:32:54 [INFO] [scene] auxiliary_info ready for 100 videos (heuristic=100)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
2026-02-11 17:37:05 [INFO] [background_consistency] prefix fallback expanded 900 unresolved entries to 100 video assignments
2026-02-11 17:37:05 [INFO] [background_consistency] skipped 900 unresolved video ids from raw VBench output
2026-02-11 17:37:05 [INFO] [appearance_style] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-11 17:38:30 [INFO] [object_class] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-11 17:38:34 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 17:38:34 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 17:39:05 [INFO] [subject_consistency] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/subject_consistency_full_info.json
2026-02-11 17:39:05 [INFO] [subject_consistency] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/subject_consistency_full_info.json
2026-02-11 17:39:05 [INFO] [subject_consistency] /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/input_videos/subject_consistency_cat_firstframes_videos has already been created, please check the path
2026-02-11 17:39:05 [INFO] [subject_consistency] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/subject_consistency_eval_results.json
2026-02-11 17:39:05 [INFO] [subject_consistency] prefix fallback expanded 900 unresolved entries to 100 video assignments
2026-02-11 17:39:05 [INFO] [subject_consistency] skipped 900 unresolved video ids from raw VBench output
2026-02-11 17:39:05 [INFO] Parsed 100 results for subject_consistency
2026-02-11 17:39:05 [INFO] Evaluating subtask: temporal_style
2026-02-11 17:43:21 [INFO] [temporal_style] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/temporal_style_full_info.json
2026-02-11 17:43:21 [INFO] [temporal_style] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/temporal_style_full_info.json
2026-02-11 17:43:21 [INFO] [temporal_style] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/temporal_style_eval_results.json
2026-02-11 17:43:21 [INFO] Parsed 100 results for temporal_style
2026-02-11 17:43:21 [INFO] Evaluating subtask: multiple_objects
2026-02-11 17:43:21 [INFO] [multiple_objects] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-11 17:43:25 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 17:43:25 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 17:47:52 [INFO] [spatial_relationship] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-11 17:47:57 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 17:47:57 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 18:01:43 [INFO] Rank 2 wrote partial results: outputs/Exp-C_OscHead_RadicalKV/vbench_partials/rank_2.csv
2026-02-11 19:16:15 [INFO] [color] auxiliary_info ready for 100 videos (heuristic=100)
2026-02-11 19:16:19 [INFO] [DetectionCheckpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 19:16:19 [INFO] [Checkpointer] Loading from /root/.cache/vbench/grit_model/grit_b_densecap_objectdet.pth ...
2026-02-11 19:25:58 [INFO] [multiple_objects] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/multiple_objects_full_info.json
2026-02-11 19:25:58 [INFO] [multiple_objects] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/multiple_objects_full_info.json
2026-02-11 19:25:58 [INFO] [multiple_objects] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/multiple_objects_eval_results.json
2026-02-11 19:25:58 [INFO] Parsed 100 results for multiple_objects
2026-02-11 19:25:58 [INFO] Evaluating subtask: overall_consistency
2026-02-11 19:30:57 [INFO] [overall_consistency] Evaluation meta data saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/overall_consistency_full_info.json
2026-02-11 19:30:57 [INFO] [overall_consistency] cur_full_info_path: /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/overall_consistency_full_info.json
2026-02-11 19:30:57 [INFO] [overall_consistency] Evaluation results saved to /workspace/t2v-eval/outputs/Exp-C_OscHead_RadicalKV/vbench_results/overall_consistency_eval_results.json
2026-02-11 19:30:57 [INFO] Parsed 100 results for overall_consistency
2026-02-11 19:30:57 [INFO] Rank 0 wrote partial results: outputs/Exp-C_OscHead_RadicalKV/vbench_partials/rank_0.csv
2026-02-11 19:47:11 [INFO] Rank 1 wrote partial results: outputs/Exp-C_OscHead_RadicalKV/vbench_partials/rank_1.csv
Traceback (most recent call last):
  File "/workspace/t2v-eval/scripts/run_vbench.py", line 20, in <module>
    main()
  File "/workspace/t2v-eval/scripts/vbench_runner/core.py", line 829, in main
    barrier_fn()
  File "/workspace/t2v-eval/scripts/vbench_runner/distributed.py", line 127, in _barrier
    raise TimeoutError(
TimeoutError: File barrier timeout at outputs/Exp-C_OscHead_RadicalKV/vbench_sync/a87ed8d8-775f-437f-a749-c29af4c1ab54/stage_002 (3/4 ready)
W0211 20:01:44.774000 139572213564544 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1912647 closing signal SIGTERM
W0211 20:01:44.776000 139572213564544 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1912648 closing signal SIGTERM
W0211 20:01:44.777000 139572213564544 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1912650 closing signal SIGTERM
E0211 20:01:45.393000 139572213564544 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 1912649) of binary: /workspace/t2v-eval/.venv/bin/python3
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/t2v-eval/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 905, in <module>
    main()
  File "/workspace/t2v-eval/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/workspace/t2v-eval/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/workspace/t2v-eval/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/workspace/t2v-eval/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/workspace/t2v-eval/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/workspace/t2v-eval/scripts/run_vbench.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-11_20:01:44
  host      : 31e00ce47b71
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1912649)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
